\section{Numerical Optimization}

In GRAPE the dimension of the optimization space is $N = \lfloor T / \Delta t \rfloor$ due to the discretization. However, most optimal solutions require much fewer degrees of freedom than introduced in the discretization. Therefore, expanding the control in a proper basis can significantly reduce the dimension of the optimization space.
The GROUP algorithm utilizes the gradient-based optimization from GRAPE while employing a chopped basis parametrization 
\begin{equation}
	u(t) = u_0 (t) + S(t) \sum_{n=1}^{M} c_n f_n (t) \; . \label{eq:controlParametrization}
\end{equation}
Here, $f_n$'s are basis functions, the coefficients $c_n$'s are the optimization parameters, $u_0 (t)$ is the initial seed, and $S(t)$ is a shape function enforcing the boundary conditions of the original control. The derivative of the cost function in the parameterization can be derived using the chain rule
\begin{equation}
	\frac{\partial J }{\partial c_n} = \sum_{j = 1}^{N} \frac{\partial J }{\partial u(t_j)} S(t_j) f_n(t_j) \; . \label{eq:GROUPgradient} 
\end{equation}
The cost of calculating the partial derivative of eq. \eqref{eq:GROUPgradient} is dominated by the time evolution of the states $\psi$ and $\chi$ from eq. \eqref{eq:CostDeriv}. Hence, the computation time of the parametrized gradient is comparable to that of GRAPE.


\section{IPOPT}

Optimal control problems are often subjected to a series of constraints, which can arise from either physical or technical limitations. The Bose-Hubbard model is only well-defined within the tight-binding limit, where the Wannier functions only overlap with their nearest neighbour~\cite{bloch2008many}. This equated to a minimum lattice depth for which the model is accurate, which in turn puts a lower bound on the interaction $U(t)$.
Interior point methods are powerful algorithms for solving constrained, non-linear, multi-variable problems~\cite{NocedalWright2006}. The methods update the control parameters using the derivatives of the cost function via Newtons methods, however, they differ from the standard Newton approach in many ways to accommodate constraints~\cite{Karmarkar1984,Wachter2006}. In addition, they provide efficient performance while having better theoretical properties than the standard simplex method~\cite{NocedalWright2006}.\\ 

Generally, the efficiency of optimization methods drops when optimizing in areas near the constraints, as only small steps can be taken there without violating the constraints. Therefore, interior point methods bias the search direction, such that the path traced in the optimization space lies well within the interior of the feasible region, whereby much larger steps can be taken~\cite{NocedalWright2006}. This is achieved by solving a series of logarithmic barrier subproblems with a variable barrier pre-factor, $\mu$, which converges to the optimal point as $\mu \to 0$~\cite{Byrd1997,Fiacco1990}. Thus, the optimization problem reads  
 \begin{align*}
	\min_{U(t) , s} \; & \; \mathcal{J} - \mu \sum_{i=1}^{N} \log s_i \\
	\text{s.t.}  \; & \; U(t) - U_{\mathrm{min}} = s  \; , 
\end{align*}
where $s$ is a vector of slack variables transforming the inequality constraints into equality constraints. The barrier term enforces the slack variables to remain positive, as the function diverges if any $s_i \to 0$. The barrier term 
Thus, the optimization Lagrangian of the problem reads
\begin{equation}
	\mathcal{L} = \mathcal{J} - \lambda ^T (U(t) - U_{\mathrm{min}} - s)
\end{equation}
where $\lambda$ is a vector of Lagrange multipliers of the inequality constraints.
Interior point methods propagate by performing a Newton step in the direction of a point, $(U^*(t), s^*, \lambda ^*)$, realizing the Karush-Kuhn-Tucker conditions~\cite{NocedalWright2006,Kuhn2014}. Expressing the conditions into a single mapping yields
\begin{equation}
	F \equiv 
	\begin{bmatrix}
  \nabla_{U(t)} \mathcal{J} - A^{T} \lambda  \\
  S \lambda - \mu e \\
  U(t) - U_{\mathrm{min}} - s 
  \end{bmatrix}
  \; \; , \; \; F(U^*(t), s^*, \lambda ^*) = 0 \; ,
\end{equation}
where $A$ is the inequality constraint Jacobian matrix, $S$ is a diagonal matrix with entries given by the vectors $s$, while we let $e = (1 ,1 , \ldots , 1 )^T$.
Applying Newtons method to the non-linear problem given by the above mapping yields
\begin{equation}
  \begin{bmatrix}
  \Laplace_{U(t)} \mathcal{L} 	& 0 	& -A^{T}	\\
  0 						& Z 			& S 			\\
  A 					& -I			& 0				 
  \end{bmatrix}  
  \begin{bmatrix}
  p_{U(t)} \\ p_s \\ p_{\lambda} 
  \end{bmatrix}
  = - F
\end{equation}
where $p = ( p_x , p_s , p_{\lambda} )$ is the constrained Newton step direction, while $Z$ is a diagonal matrix with entries given by the vectors $\lambda$, and $I$ is the identity.
After computing the step direction, the parameters are iterated according to 
\begin{equation}
\begin{bmatrix}
  U^{k+1} \\ s^{k+1} \\ \lambda^{k+1} 
\end{bmatrix} 
=
\begin{bmatrix}
  U^{k} \\ s^{k} \\ \lambda^{k} 
\end{bmatrix} 
+ \alpha \circ p \; ,
\label{eq:ConNewtonStep}
\end{equation}
where $\alpha$ is a vector of step-sizes for each of the variable, which are determined via a line-search \cite{Wachter2005,Fletcher2002}.

In the GRAPE formalism, the interaction, $U(t)$, serves as the control function, whereby the constraint Jacobian is simply the identity. Applying a parameterization of the control function like GROUP causes 